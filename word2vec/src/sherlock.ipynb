{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector representation of word\n",
    " Tham khao *Patrick Coady (pcoady@alum.mit.edu)*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordvector import WordVector\n",
    "from windowmodel import WindowModel\n",
    "import docload\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Books, Build Dictionary & Convert Books to Integer Vector\n",
    "Input: 3 file .txt \n",
    "1. adventures_of_sherlock_holmes.txt\n",
    "2. hound_of_the_baskervilles.txt\n",
    "3. sign_of_the_four.txt\n",
    "\n",
    "Load the books and build a dictionary of all unique words. The dictionary maps each unique word to an integer. All words are converted to lower case. And punctuation are treated as words (i.e. \" , . ? and !). If the size of the book vocabulary exceeds the pre-set limit (**vocab_size**), then the most infrequent words are mapped to the last integer in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded and processed: 24080 lines, 247812 words.\n"
     ]
    }
   ],
   "source": [
    "files = ['../data/adventures_of_sherlock_holmes.txt',\n",
    "        '../data/hound_of_the_baskervilles.txt',\n",
    "        '../data/sign_of_the_four.txt']\n",
    "word_array, dictionary, num_lines, num_words = docload.build_word_array(\n",
    "    files, vocab_size=50000, gutenberg=True)\n",
    "\n",
    "print('Document loaded and processed: {} lines, {} words.'\n",
    "      .format(num_lines, num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training set ...\n",
      "Training set built.\n",
      "Model built. Vocab size = 11756. Document length = 247812 words.\n",
      "Training ...\n",
      "End Training: total batches = 836280. train loss = 1.40, val loss = 1.65\n"
     ]
    }
   ],
   "source": [
    "print('Building training set ...')\n",
    "x, y = WindowModel.build_training_set(word_array)\n",
    "\n",
    "# shuffle and split 10% validation data\n",
    "x_shuf, y_shuf = sklearn.utils.shuffle(x, y, random_state=0)\n",
    "split = round(x_shuf.shape[0]*0.9)\n",
    "x_val, y_val = (x_shuf[split:, :], y_shuf[split:, :])\n",
    "x_train, y_train = (x[:split, :], y[:split, :])\n",
    "\n",
    "print('Training set built.')\n",
    "graph_params = {'batch_size': 32,\n",
    "                'vocab_size': np.max(x)+1,\n",
    "                'embed_size': 64,\n",
    "                'hid_size': 64,\n",
    "                'neg_samples': 64,\n",
    "                'learn_rate': 0.01,\n",
    "                'momentum': 0.9,\n",
    "                'embed_noise': 0.1,\n",
    "                'hid_noise': 0.3,\n",
    "                'optimizer': 'Momentum'}\n",
    "model = WindowModel(graph_params)\n",
    "print('Model built. Vocab size = {}. Document length = {} words.'\n",
    "      .format(np.max(x)+1, len(word_array)))\n",
    "\n",
    "print('Training ...')\n",
    "results = model.train(x_train, y_train, x_val, y_val, epochs=120, verbose=False)\n",
    "\n",
    "word_vector_embed = WordVector(results['embed_weights'], dictionary)\n",
    "word_vector_nce = WordVector(results['nce_weights'], dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100 Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', '\"', 'and', 'i', 'of', 'to', 'a', 'that', 'it', 'in', 'he', 'you', 'was', '-', 'his', 'is', 'my', 'have', 'had', 'with', 'as', 'at', '?', 'for', 'which', 'we', 'but', 'be', 'not', 'me', 'this', 'there', 'upon', 'him', 'said', 'from', 'so', 'no', 'on', 'one', 'all', 'holmes', 'been', 'her', 'were', 'what', 'very', 'by', 'your', 'an', 'she', 'are', 'would', '!', 'man', 'out', 'could', 'then', 'if', 'our', 'up', 'when', 'has', 'do', 'will', \"'\", 'us', 'who', 'some', 'into', 'sir', 'now', 'see', 'down', 'they', 'or', 'should', 'little', 'mr', 'well', 'more', 'over', 'can', 'may', 'know', 'about', 'am', 'think', 'them', 'only', 'must', ';', 'did', 'here', 'come', 'time', 'than', 'how']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_embed.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarities\n",
    "The model learns 2 word vector representations. \n",
    "1. The embedding vector from the one-hot input\n",
    "2. The vector from the hidden layer to the network output\n",
    "\n",
    "In general, the output layer vector seems to learn more meaningful vector representation of words. We quickly check the closest words (cosine similarity) to the word \"seven\". Remember, this model had no human-labeled data or any data sources outside of the raw book text. The hidden layer to output matrix correctly finds that other numbers are most similar to \"seven\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer: 8 closest words to: 'seven'\n",
      "['eight', 'ten', 'four', 'sixty', 'five', 'arc', 'straining', 'wandered'] \n",
      "\n",
      "Hidden-to-output layer: 8 closest words to: 'seven'\n",
      "['eight', 'thirty', 'twenty', 'ten', 'eleven', 'five', 'zero', 'twelve']\n"
     ]
    }
   ],
   "source": [
    "word = \"seven\"\n",
    "print('Embedding layer: 8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_embed.n_closest(word=word, num_closest=8, metric='cosine'), '\\n')\n",
    "print('Hidden-to-output layer: 8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=8, metric='cosine'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Performance of *embed_weights* vs. *nce_weights*  \n",
    "\n",
    "From a qualitative perspective, the *nce_weights* consistently give more meaningful results when asking for similar words. Although, they both do OK on suggesting similar words to \"seven\". Also, for the analogy task (e.g. A is to B, as C is to ?) the *nce_weights* give more \"sensible\" results.  \n",
    "\n",
    "Clearly, the *embed_weights* are learning something. These weights are the first stage in the model, and the model wouldn't perform if they were learning nonsense. \n",
    "\n",
    "Cosine similarity is used as a distance metric for running similarity and analogy tasks. It might be interesting to experiment with other distance measures. A quick look at Euclidean distance was not promising. The code supports 15+ different distance metrics from *scipy.spatial.distance*, some experimentation here might be interesting.  \n",
    "\n",
    "That said, to avoid clutter, the rest of this notebook will focus on the word vectors from the *nce_weights* matrix and use cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 closest words to: 'laughing'\n",
      "['pensively', 'yawning', 'chuckling', 'lightly', 'languidly', 'dryly', 'earnestly', 'underneath']\n"
     ]
    }
   ],
   "source": [
    "word = \"laughing\"\n",
    "print('8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=8, metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 closest words to: 'mr'\n",
      "['mrs', 'st', 'dr', 'others', \"'mr\", 'c', 'l', 'reading']\n"
     ]
    }
   ],
   "source": [
    "word = \"mr\"\n",
    "print('8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=8, metric='cosine'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogies  \n",
    "\n",
    "Because words are represented as vectors, it is interesting to try vector addition to predict the 4th word in an analogy. The premise being that A is to B as C is to D can be represented as:  $\\mathbf{x_d}=\\mathbf{x_b}-\\mathbf{x_a}+\\mathbf{x_c}$.  \n",
    "\n",
    "![](notebook_images/analogies.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'was', 'has', 'looks', 'seems']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_nce.analogy('had', 'has', 'was', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['forehead', 'arm', 'cheeks', 'forearm', 'boots']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_nce.analogy('boot', 'boots', 'arm', 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Replacement Words in a Passage\n",
    "\n",
    "For fun, I took a random 200 word passage and used the network to make predictions to replace all the words. The results has some semblance of grammar, but is mostly nonsensical. This is to be expected, the model only uses the 2 preceding and 2 following words to make predictions. A Recurrent NN is a more appropriate tool for this, but here it is anyway:\n",
    "\n",
    "#### Original Passage\n",
    "\n",
    "**well,  it is just as i have been telling you,  mr.  sherlock holmes, \" said jabez wilson,  mopping his forehead\" i have a small pawnbroker's business at coburg square,  near the city.**\n",
    "\n",
    "#### Reconstructed Passage\n",
    "\n",
    "**oh,  it was,  as i have been told you,  mr.  sherlock holmes, \" said sherlock wilson,  upon his? \" i am a small public business at coburg square,  with the time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab 100 word passage from book\n",
    "reverse_dict = word_vector_nce.get_reverse_dict()\n",
    "passage = [x for x in map(lambda x: reverse_dict[x], word_array[12200:12300])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",  at eleven o'clock,  to duncan ross,  at the offices of the league,  7 pope's court,  fleet street. \"\" what on earth does this mean? \" i ejaculated after i had twice read over the extraordinary announcement.  holmes chuckled and wriggled in his chair,  as was his habit when in high spirits. \" it is a little off the beaten track,  isn't it? \" said he. \" and now,  mr.  wilson,  off you go at scratch and tell us all about yourself,  your\n"
     ]
    }
   ],
   "source": [
    "# print passage with some crude formatting (e.g. space after comma)\n",
    "readable = ''\n",
    "for word in passage:\n",
    "    if word == '\"':\n",
    "        readable += word\n",
    "    elif word in ['?', '!', '.', ',']:\n",
    "        readable += word + ' '\n",
    "    else: \n",
    "        readable += ' ' + word\n",
    "print(readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model-save/model_save-120\n"
     ]
    }
   ],
   "source": [
    "# use model to replace words in original passage with predicted words\n",
    "# need to grab 2 words before and after passage\n",
    "x, y = WindowModel.build_training_set(word_array[(12200-2):(12300+2)])\n",
    "y_hat = model.predict(x, 120)\n",
    "passage_predict = [x for x in map(lambda x: reverse_dict[x], y_hat[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",  after two o'clock,  to step baskerville,  at the time of the night,  though pope's court,  oxford street. \"\" what on earth will it be? \" i thought after i could just right on the old open.  holmes rose and away in his death,  there in his wife,  in black street. \" it is a man from the low door,  is he? \" said holmes. \" and now,  mr.  holmes,  when you go at last and give you all for it,  your\n"
     ]
    }
   ],
   "source": [
    "# print predicted passage\n",
    "readable = ''\n",
    "for word in passage_predict:\n",
    "    if word == '\"':\n",
    "        readable += word\n",
    "    elif word in ['?', '!', '.', ',']:\n",
    "        readable += word + ' '\n",
    "    else: \n",
    "        readable += ' ' + word\n",
    "print(readable)\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'was', 'over', 'becomes', 'does']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_nce.analogy('were', 'is', 'was', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 closest words to: ','\n",
      "['!', ';', '.', '?', 'illegibly', ':', 'chimerical', 'trove']\n",
      "Hidden-to-output layer: 8 closest words to: ','\n",
      "[';', '!', '?', 'doubt', ':', 'anyone', 't', 'blinds']\n"
     ]
    }
   ],
   "source": [
    "word = \",\"\n",
    "print('8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_embed.n_closest(word=word, num_closest=8, metric='cosine'))\n",
    "print('Hidden-to-output layer: 8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=8, metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built. Vocab size = 11756. Document length = 247812 words.\n",
      "number of words:  247812\n",
      "dictionary length  11756\n"
     ]
    }
   ],
   "source": [
    "# word_counter is a list: \n",
    "# dictionary is a collection:\n",
    "print('Model built. Vocab size = {}. Document length = {} words.'\n",
    "      .format(len(dictionary), len(word_array)))\n",
    "print(\"number of words: \", num_words)\n",
    "print(\"dictionary length \", len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
